# RustClaw Configuration Example
# 
# Config loading priority (highest to lowest):
# 1. Environment variables (TELEGRAM_BOT_TOKEN, OPENAI_API_KEY, etc.)
# 2. Local ./rustclaw.toml (workspace override)
# 3. Global ~/.rustclaw/rustclaw.toml (auto-created on first run)

[telegram]
bot_token = ""  # Set via TELEGRAM_BOT_TOKEN env var

[providers]
default = "openai"  # or "ollama"

[providers.openai]
api_key = ""  # Set via OPENAI_API_KEY env var
model = "gpt-4o-mini"
base_url = ""  # Optional: Set via OPENAI_BASE_URL env var

[providers.ollama]
base_url = "http://localhost:11434"
model = "llama3"

[agent]
# Maximum number of tool calls per request (prevents infinite loops)
max_tool_iterations = 10

# Context window size in tokens (for compression decisions)
context_window = 128000

# Number of recent conversation turns to keep before compression
recent_turns = 10

[database]
path = "rustclaw.db"

[logging]
level = "info"  # trace, debug, info, warn, error

# ============================================================================
# MCP (Model Context Protocol) Configuration
# ============================================================================
# MCP servers provide additional tools and capabilities to the AI agent
# Supports: stdio transport (local commands) and HTTP transport (remote servers)

[mcp]
startup_timeout = 10  # Global default timeout in seconds for MCP server startup

[mcp.servers]
# Filesystem MCP server - provides file system access (stdio transport)
# Usage: Read, write, and manage files in /tmp directory
filesystem = "npx -y @modelcontextprotocol/server-filesystem /tmp"

# GitHub MCP server - provides GitHub API access (requires GITHUB_TOKEN env var)
# Usage: Create issues, PRs, read repositories, etc.
# github = "mcp-server-github"

# Simple HTTP-based MCP server (Streamable HTTP transport)
# Usage: Connect to remote MCP servers over HTTP
# weather = "http://localhost:3000/mcp"

# Remote MCP server with Bearer token authentication
# [mcp.servers.web-search]
# url = "https://open.bigmodel.cn/api/mcp/web_search_prime/mcp"
# headers = { Authorization = "Bearer your_api_key" }

# Stdio MCP server with explicit command, args, and environment variables
# [mcp.servers.zai-mcp-server]
# command = "npx"
# args = ["-y", "@z_ai/mcp-server"]
# env = { Z_AI_API_KEY = "your_api_key", Z_AI_MODE = "ZHIPU" }

# ============================================================================
# Skills Configuration
# ============================================================================
# Skills are modular AI capabilities using progressive disclosure (2025-2026 best practices)
# - Phase 1: Skills metadata (name, description) loaded at startup
# - Phase 2: Full skill content loaded on-demand when matched
# - Supports recursive discovery in subdirectories
# 
# See examples/skills/ directory for example skills

[skills]
# Directories to scan for skills (supports multiple directories)
# Skills are Markdown files with frontmatter: name, description, trigger_patterns
directories = ["~/.rustclaw/skills", "./.rustclaw/skills", "./examples/skills"]

# ============================================================================
# Example Skills Directory Structure
# ============================================================================
# ~/.rustclaw/skills/
# ├── code-reviewer/
# │   └── skill.md          # Triggered by "review this code", "code review"
# ├── commit-messages/
# │   └── skill.md          # Triggered by "write commit message"
# ├── testing/
# │   └── skill.md          # Triggered by "write tests"
# └── brainstorming/
#     └── skill.md          # Triggered by "brainstorm", "ideate"
#
# Each skill.md contains:
# ---
# name: Code Reviewer
# description: Comprehensive code review with best practices
# trigger_patterns:
#   - "review this code"
#   - "code review"
#   - "check my code"
# ---
# [Skill content in Markdown...]
